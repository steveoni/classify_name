{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify name Based on Language(Country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from io import open\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'assets//names/Irish.txt', u'assets//names/Dutch.txt', u'assets//names/Korean.txt', u'assets//names/Italian.txt', u'assets//names/Polish.txt', u'assets//names/Scottish.txt', u'assets//names/Japanese.txt', u'assets//names/Chinese.txt', u'assets//names/English.txt', u'assets//names/Portuguese.txt', u'assets//names/Spanish.txt', u'assets//names/French.txt', u'assets//names/Arabic.txt', u'assets//names/German.txt', u'assets//names/Russian.txt', u'assets//names/Vietnamese.txt', u'assets//names/Greek.txt', u'assets//names/Czech.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('assets//names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "name_char = set()\n",
    "labels = []\n",
    "names = []\n",
    "for filename in findFiles('assets//names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    \n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    for name in lines:\n",
    "        names.append(unicodeToAscii(name))\n",
    "        labels.append(category)\n",
    "        for char in name:\n",
    "            if char not in name_char:\n",
    "                name_char.add(unicodeToAscii(char))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a character to index dict has look up table for word encoding to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_toIndex = dict(\n",
    "                    [(char, i+1) for i, char in enumerate(name_char)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert word to vecotr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns each word to vector using the char_toIndex\n",
    "#the num of rows is the length of the word array and the column is the \n",
    "#maximum length\n",
    "def createvec(text_arr,lookup_table):\n",
    "    \n",
    "    arr_length = len(text_arr)\n",
    "    max_text_len = max([len(text) for text in text_arr])\n",
    "    \n",
    "    #helps to create padding automatically\n",
    "    text_vec = np.zeros((arr_length,max_text_len),dtype='int')\n",
    "    \n",
    "    for i, text in enumerate(text_arr):\n",
    "        \n",
    "        for j, char in enumerate(text):\n",
    "            text_vec[i,j] = lookup_table[char]\n",
    "            \n",
    "            \n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tesing how it works\n",
    "text_arr = [\"Dare\",\"Dalley\",\"Boo\"]\n",
    "\n",
    "vec = createvec(text_arr,char_toIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 31, 49, 34,  0,  0],\n",
       "       [ 9, 31, 43, 43, 34, 54],\n",
       "       [ 7, 44, 44,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = createvec(names,char_toIndex)[:2000,:] #take a size of 20000 for easy batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cobvert labels to list\n",
    "classes = list(set(labels))\n",
    "\n",
    "label_dict = dict((label,i) for i, label in enumerate(classes))\n",
    "data_label = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    data_label.append(label_dict[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(data_label)[:2000] #convert label to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train, test and validation\n",
    "train_x, remaining_x,train_y,remaining_y = train_test_split(word_vec,labels,test_size=0.2,shuffle=True)\n",
    "val_x,test_x,val_y,test_y = train_test_split(remaining_x,remaining_y,test_size=0.5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(name) for name in names]) #the maximum length of the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\"\"\"\n",
    "convert the numpy into torch tensor\n",
    "and create a tensor dataset and pass\n",
    "into Dataloader to load data in batch\n",
    "\"\"\"\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y).long())\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 19])\n",
      "Sample label size:  torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClassifyRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform classify name.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(ClassifyRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "\n",
    "        new_out = lstm_out[:,-1,:] # take the last output\n",
    "        \n",
    "\n",
    "        out = self.dropout(new_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "\n",
    "        soft_out = F.log_softmax(out,dim=1)\n",
    "\n",
    "        return soft_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassifyRNN(\n",
      "  (embedding): Embedding(57, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#assign all variables needed\n",
    "vocab_size = len(name_char)+1\n",
    "output_size = len(classes)\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = ClassifyRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20... Step: 100... Loss: 1.861728... Val Loss: 1.716569\n",
      "Epoch: 7/20... Step: 200... Loss: 1.770824... Val Loss: 1.691263\n",
      "Epoch: 10/20... Step: 300... Loss: 1.741355... Val Loss: 1.693472\n",
      "Epoch: 13/20... Step: 400... Loss: 1.278846... Val Loss: 1.301431\n",
      "Epoch: 16/20... Step: 500... Loss: 0.719358... Val Loss: 0.736529\n",
      "Epoch: 19/20... Step: 600... Loss: 0.487323... Val Loss: 0.640174\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 20 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.603\n",
      "Test accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.long())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    _,pred = torch.max(output,1)\n",
    "    \n",
    "#     print(\"pred shape\", pred.size())\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.long().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vec(text_arr,lookup_table):\n",
    "    \n",
    "    arr_length = len(text_arr)\n",
    "#     max_text_len = max([len(text) for text in text_arr])\n",
    "    \n",
    "    text_vec = np.zeros((arr_length,max_length),dtype='int')\n",
    "    \n",
    "    for i, text in enumerate(text_arr):\n",
    "        \n",
    "        for j, char in enumerate(text):\n",
    "            text_vec[i,j] = lookup_table[char]\n",
    "            \n",
    "            \n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Portuguese',\n",
       " u'Irish',\n",
       " u'Spanish',\n",
       " u'Czech',\n",
       " u'Chinese',\n",
       " u'Vietnamese',\n",
       " u'German',\n",
       " u'Dutch',\n",
       " u'Japanese',\n",
       " u'French',\n",
       " u'Greek',\n",
       " u'Scottish',\n",
       " u'Russian',\n",
       " u'Polish',\n",
       " u'Korean',\n",
       " u'Arabic',\n",
       " u'English',\n",
       " u'Italian']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,name, max_length=19):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    test_ints = test_vec([name],char_toIndex)\n",
    "    \n",
    "    print(\"test_ints\", test_ints.shape)\n",
    "    feature_tensor = torch.from_numpy(test_ints)\n",
    "    \n",
    "    print(\"feature tensor\", feature_tensor.size())\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    print(torch.exp(output))\n",
    "    _,pred = torch.max(output,1)\n",
    "    # printing output value, before rounding\n",
    "#     print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    print(pred)\n",
    "    print(classes[pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ints (1, 19)\n",
      "feature tensor torch.Size([1, 19])\n",
      "tensor([[3.0968e-06, 2.0372e-04, 1.6461e-06, 2.3560e-06, 1.1727e-06, 2.3503e-06,\n",
      "         1.0145e-06, 2.7222e-04, 9.9767e-01, 2.9914e-06, 1.6593e-06, 1.3290e-04,\n",
      "         2.1683e-06, 3.7159e-04, 7.1800e-04, 1.6021e-06, 1.9931e-06, 6.1332e-04]],\n",
      "       device='cuda:0', grad_fn=<ExpBackward>)\n",
      "tensor([8], device='cuda:0')\n",
      "Japanese\n"
     ]
    }
   ],
   "source": [
    "predict(net,\"Arishima\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
